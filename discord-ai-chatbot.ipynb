{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:19.530664Z","iopub.execute_input":"2021-09-12T15:21:19.530980Z","iopub.status.idle":"2021-09-12T15:21:27.543067Z","shell.execute_reply.started":"2021-09-12T15:21:19.530905Z","shell.execute_reply":"2021-09-12T15:21:27.542277Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.7.1+cu110)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch) (1.19.5)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (3.7.4.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"import glob\nimport logging\nimport os\nimport pickle\nimport random\nimport re\nimport shutil\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.utils.data.distributed import DistributedSampler\nfrom tqdm.notebook import tqdm, trange\n\nfrom pathlib import Path\n\nfrom transformers import (\n    MODEL_WITH_LM_HEAD_MAPPING,\n    WEIGHTS_NAME,\n    AdamW,\n    AutoConfig,\n    PreTrainedModel,\n    PreTrainedTokenizer,\n    get_linear_schedule_with_warmup,\n)\n\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-09-12T15:21:27.546613Z","iopub.execute_input":"2021-09-12T15:21:27.546840Z","iopub.status.idle":"2021-09-12T15:21:37.845323Z","shell.execute_reply.started":"2021-09-12T15:21:27.546813Z","shell.execute_reply":"2021-09-12T15:21:37.844608Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2021-09-12 15:21:33.437378: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n","output_type":"stream"}]},{"cell_type":"code","source":"data = pd.read_csv('../input/the-office-us-complete-dialoguetranscript/The-Office-Lines-V4.csv')\ndata","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:37.846596Z","iopub.execute_input":"2021-09-12T15:21:37.846862Z","iopub.status.idle":"2021-09-12T15:21:38.009000Z","shell.execute_reply.started":"2021-09-12T15:21:37.846831Z","shell.execute_reply":"2021-09-12T15:21:38.008194Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"       season  episode   title  scene   speaker  \\\n0           1        1   Pilot      1   Michael   \n1           1        1   Pilot      1       Jim   \n2           1        1   Pilot      1   Michael   \n3           1        1   Pilot      1       Jim   \n4           1        1   Pilot      1   Michael   \n...       ...      ...     ...    ...       ...   \n54621       9       24  Finale   8153     Creed   \n54622       9       24  Finale   8154  Meredith   \n54623       9       24  Finale   8155   Phyllis   \n54624       9       24  Finale   8156       Jim   \n54625       9       24  Finale   8157       Pam   \n\n                                                    line Unnamed: 6  \n0      All right Jim. Your quarterlies look very good...        NaN  \n1             Oh, I told you. I couldn't close it. So...        NaN  \n2      So you've come to the master for guidance? Is ...        NaN  \n3             Actually, you called me in here, but yeah.        NaN  \n4        All right. Well, let me show you how it's done.        NaN  \n...                                                  ...        ...  \n54621  It all seems so very arbitrary. I applied for ...        NaN  \n54622  I just feel lucky that I got a chance to share...        NaN  \n54623  I'm happy that this was all filmed so I can re...        NaN  \n54624  I sold paper at this company for 12 years. My ...        NaN  \n54625  I thought it was weird when you picked us to m...        NaN  \n\n[54626 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>episode</th>\n      <th>title</th>\n      <th>scene</th>\n      <th>speaker</th>\n      <th>line</th>\n      <th>Unnamed: 6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Michael</td>\n      <td>All right Jim. Your quarterlies look very good...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Jim</td>\n      <td>Oh, I told you. I couldn't close it. So...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Michael</td>\n      <td>So you've come to the master for guidance? Is ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Jim</td>\n      <td>Actually, you called me in here, but yeah.</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Michael</td>\n      <td>All right. Well, let me show you how it's done.</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>54621</th>\n      <td>9</td>\n      <td>24</td>\n      <td>Finale</td>\n      <td>8153</td>\n      <td>Creed</td>\n      <td>It all seems so very arbitrary. I applied for ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>54622</th>\n      <td>9</td>\n      <td>24</td>\n      <td>Finale</td>\n      <td>8154</td>\n      <td>Meredith</td>\n      <td>I just feel lucky that I got a chance to share...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>54623</th>\n      <td>9</td>\n      <td>24</td>\n      <td>Finale</td>\n      <td>8155</td>\n      <td>Phyllis</td>\n      <td>I'm happy that this was all filmed so I can re...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>54624</th>\n      <td>9</td>\n      <td>24</td>\n      <td>Finale</td>\n      <td>8156</td>\n      <td>Jim</td>\n      <td>I sold paper at this company for 12 years. My ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>54625</th>\n      <td>9</td>\n      <td>24</td>\n      <td>Finale</td>\n      <td>8157</td>\n      <td>Pam</td>\n      <td>I thought it was weird when you picked us to m...</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>54626 rows Ã— 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data.rename(columns={'speaker': 'name'}, inplace=True)\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.011187Z","iopub.execute_input":"2021-09-12T15:21:38.011471Z","iopub.status.idle":"2021-09-12T15:21:38.024933Z","shell.execute_reply.started":"2021-09-12T15:21:38.011434Z","shell.execute_reply":"2021-09-12T15:21:38.024013Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   season  episode  title  scene     name  \\\n0       1        1  Pilot      1  Michael   \n1       1        1  Pilot      1      Jim   \n2       1        1  Pilot      1  Michael   \n3       1        1  Pilot      1      Jim   \n4       1        1  Pilot      1  Michael   \n\n                                                line Unnamed: 6  \n0  All right Jim. Your quarterlies look very good...        NaN  \n1         Oh, I told you. I couldn't close it. So...        NaN  \n2  So you've come to the master for guidance? Is ...        NaN  \n3         Actually, you called me in here, but yeah.        NaN  \n4    All right. Well, let me show you how it's done.        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>season</th>\n      <th>episode</th>\n      <th>title</th>\n      <th>scene</th>\n      <th>name</th>\n      <th>line</th>\n      <th>Unnamed: 6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Michael</td>\n      <td>All right Jim. Your quarterlies look very good...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Jim</td>\n      <td>Oh, I told you. I couldn't close it. So...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Michael</td>\n      <td>So you've come to the master for guidance? Is ...</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Jim</td>\n      <td>Actually, you called me in here, but yeah.</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Pilot</td>\n      <td>1</td>\n      <td>Michael</td>\n      <td>All right. Well, let me show you how it's done.</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(data)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.026111Z","iopub.execute_input":"2021-09-12T15:21:38.026437Z","iopub.status.idle":"2021-09-12T15:21:38.036304Z","shell.execute_reply.started":"2021-09-12T15:21:38.026405Z","shell.execute_reply":"2021-09-12T15:21:38.035513Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"54626"},"metadata":{}}]},{"cell_type":"code","source":"CHARACTER_NAME = 'Pam'\nsum(data.name == CHARACTER_NAME)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.037862Z","iopub.execute_input":"2021-09-12T15:21:38.038121Z","iopub.status.idle":"2021-09-12T15:21:38.060065Z","shell.execute_reply.started":"2021-09-12T15:21:38.038091Z","shell.execute_reply":"2021-09-12T15:21:38.059250Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"4973"},"metadata":{}}]},{"cell_type":"code","source":"contexted = []\n\n# context window of size 7\nn = 7\n\nfor i in data[data.name == CHARACTER_NAME].index:\n  if i < n:\n    continue\n  row = []\n  prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces  \n  for j in range(i, prev, -1):\n    row.append(data.line[j])\n  contexted.append(row)\n\ncolumns = ['response', 'context'] \ncolumns = columns + ['context/' + str(i) for i in range(n - 1)]\n\ndf = pd.DataFrame.from_records(contexted, columns=columns)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.061511Z","iopub.execute_input":"2021-09-12T15:21:38.061748Z","iopub.status.idle":"2021-09-12T15:21:38.522863Z","shell.execute_reply.started":"2021-09-12T15:21:38.061717Z","shell.execute_reply":"2021-09-12T15:21:38.522164Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.525909Z","iopub.execute_input":"2021-09-12T15:21:38.526421Z","iopub.status.idle":"2021-09-12T15:21:38.539223Z","shell.execute_reply.started":"2021-09-12T15:21:38.526393Z","shell.execute_reply":"2021-09-12T15:21:38.538603Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                response                                            context  \\\n0    Well. I don't know.  I've, uh, I've been at Dunder Mifflin for 12 y...   \n1                  What?  If you think she's cute now, you should have s...   \n2  Uh, yeah. Just a fax.                                      Any messages?   \n3   You haven't told me.  Oh! Pam, this is from Corporate. How many time...   \n4   Uh, yeah, the one...  Really? I didn't...  Did we get a fax this mor...   \n\n                                           context/0  \\\n0   Yes, I'd like to speak to your office manager...   \n1                                Well. I don't know.   \n2                                              What?   \n3                              Uh, yeah. Just a fax.   \n4        Well, I faxed one over to you this morning.   \n\n                                           context/1  \\\n0    All right. Well, let me show you how it's done.   \n1  I've, uh, I've been at Dunder Mifflin for 12 y...   \n2  If you think she's cute now, you should have s...   \n3                                      Any messages?   \n4                           I didn't get any agenda.   \n\n                                           context/2  \\\n0         Actually, you called me in here, but yeah.   \n1   Yes, I'd like to speak to your office manager...   \n2                                Well. I don't know.   \n3                                              What?   \n4                                   What? I'm sorry?   \n\n                                           context/3  \\\n0  So you've come to the master for guidance? Is ...   \n1    All right. Well, let me show you how it's done.   \n2  I've, uh, I've been at Dunder Mifflin for 12 y...   \n3  If you think she's cute now, you should have s...   \n4                         Um... Me no get an agenda.   \n\n                                           context/4  \\\n0         Oh, I told you. I couldn't close it. So...   \n1         Actually, you called me in here, but yeah.   \n2   Yes, I'd like to speak to your office manager...   \n3                                Well. I don't know.   \n4  Alright, was there anything you wanted to add ...   \n\n                                           context/5  \n0  All right Jim. Your quarterlies look very good...  \n1  So you've come to the master for guidance? Is ...  \n2    All right. Well, let me show you how it's done.  \n3  I've, uh, I've been at Dunder Mifflin for 12 y...  \n4  Corporate really doesn't really interfere with...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>context</th>\n      <th>context/0</th>\n      <th>context/1</th>\n      <th>context/2</th>\n      <th>context/3</th>\n      <th>context/4</th>\n      <th>context/5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Well. I don't know.</td>\n      <td>I've, uh, I've been at Dunder Mifflin for 12 y...</td>\n      <td>Yes, I'd like to speak to your office manager...</td>\n      <td>All right. Well, let me show you how it's done.</td>\n      <td>Actually, you called me in here, but yeah.</td>\n      <td>So you've come to the master for guidance? Is ...</td>\n      <td>Oh, I told you. I couldn't close it. So...</td>\n      <td>All right Jim. Your quarterlies look very good...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What?</td>\n      <td>If you think she's cute now, you should have s...</td>\n      <td>Well. I don't know.</td>\n      <td>I've, uh, I've been at Dunder Mifflin for 12 y...</td>\n      <td>Yes, I'd like to speak to your office manager...</td>\n      <td>All right. Well, let me show you how it's done.</td>\n      <td>Actually, you called me in here, but yeah.</td>\n      <td>So you've come to the master for guidance? Is ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Uh, yeah. Just a fax.</td>\n      <td>Any messages?</td>\n      <td>What?</td>\n      <td>If you think she's cute now, you should have s...</td>\n      <td>Well. I don't know.</td>\n      <td>I've, uh, I've been at Dunder Mifflin for 12 y...</td>\n      <td>Yes, I'd like to speak to your office manager...</td>\n      <td>All right. Well, let me show you how it's done.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You haven't told me.</td>\n      <td>Oh! Pam, this is from Corporate. How many time...</td>\n      <td>Uh, yeah. Just a fax.</td>\n      <td>Any messages?</td>\n      <td>What?</td>\n      <td>If you think she's cute now, you should have s...</td>\n      <td>Well. I don't know.</td>\n      <td>I've, uh, I've been at Dunder Mifflin for 12 y...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Uh, yeah, the one...</td>\n      <td>Really? I didn't...  Did we get a fax this mor...</td>\n      <td>Well, I faxed one over to you this morning.</td>\n      <td>I didn't get any agenda.</td>\n      <td>What? I'm sorry?</td>\n      <td>Um... Me no get an agenda.</td>\n      <td>Alright, was there anything you wanted to add ...</td>\n      <td>Corporate really doesn't really interfere with...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"trn_df, val_df = train_test_split(df, test_size=0.1)\ntrn_df.head()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.540827Z","iopub.execute_input":"2021-09-12T15:21:38.541478Z","iopub.status.idle":"2021-09-12T15:21:38.565496Z","shell.execute_reply.started":"2021-09-12T15:21:38.541440Z","shell.execute_reply":"2021-09-12T15:21:38.564814Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                                               response  \\\n989                                       Okay. Angela?   \n2634                                         Yes I did.   \n2668   How could you do this to me? He's my boss! Ho...   \n3808  Okay, I know I've been crying easily today, bu...   \n896                              Hey, Ryan, are you ok?   \n\n                                                context  \\\n989                                               Cool.   \n2634  I know you specifically put this song on your ...   \n2668  ...the blood is gushing down, the blood is gus...   \n3808          Okay. Oh, uh, you dropped something. Jim?   \n896   Yes! How are you going to cook every meal of t...   \n\n                                              context/0  \\\n989   Sssssso, what we need to do... is to forget ab...   \n2634                                            Mm-hmm.   \n2668    I'm sleeping with Pam's mom. Sometimes, dinner.   \n3808             Alright, I'm gonna go warm up the car.   \n896                                     Three kitchens?   \n\n                                              context/1  \\\n989               Why would anyone go to jail for that?   \n2634                              I begged them not to.   \n2668  There's a knife in your back and the blood is ...   \n3808                                             Night.   \n896         Most apartments these days have like three.   \n\n                                              context/2  \\\n989   In our society, a black man can be arrested fo...   \n2634                                   Wear a tie much?   \n2668                                        Feels good.   \n3808                                        Good night.   \n896                                    It's actually...   \n\n                                              context/3  \\\n989                               I wonder what he did.   \n2634                                    You look great.   \n2668  There's an egg on your head and the yolk is ru...   \n3808                                   Good night Andy.   \n896      Wow, you got totally taken for a ride Beesley.   \n\n                                              context/4  \\\n989   Why did the convict have to be a black guy? It...   \n2634                                    You look great.   \n2668                                              Okay.   \n3808                                               Bye.   \n896                                 I have one kitchen.   \n\n                                              context/5  \n989                                            Michael?  \n2634   Yes! Yes! I have so much joy... in my heart.....  \n2668   You seem tense. Hey, you want me to give you ...  \n3808                                             Night.  \n896                              And how many kitchens?  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>context</th>\n      <th>context/0</th>\n      <th>context/1</th>\n      <th>context/2</th>\n      <th>context/3</th>\n      <th>context/4</th>\n      <th>context/5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>989</th>\n      <td>Okay. Angela?</td>\n      <td>Cool.</td>\n      <td>Sssssso, what we need to do... is to forget ab...</td>\n      <td>Why would anyone go to jail for that?</td>\n      <td>In our society, a black man can be arrested fo...</td>\n      <td>I wonder what he did.</td>\n      <td>Why did the convict have to be a black guy? It...</td>\n      <td>Michael?</td>\n    </tr>\n    <tr>\n      <th>2634</th>\n      <td>Yes I did.</td>\n      <td>I know you specifically put this song on your ...</td>\n      <td>Mm-hmm.</td>\n      <td>I begged them not to.</td>\n      <td>Wear a tie much?</td>\n      <td>You look great.</td>\n      <td>You look great.</td>\n      <td>Yes! Yes! I have so much joy... in my heart.....</td>\n    </tr>\n    <tr>\n      <th>2668</th>\n      <td>How could you do this to me? He's my boss! Ho...</td>\n      <td>...the blood is gushing down, the blood is gus...</td>\n      <td>I'm sleeping with Pam's mom. Sometimes, dinner.</td>\n      <td>There's a knife in your back and the blood is ...</td>\n      <td>Feels good.</td>\n      <td>There's an egg on your head and the yolk is ru...</td>\n      <td>Okay.</td>\n      <td>You seem tense. Hey, you want me to give you ...</td>\n    </tr>\n    <tr>\n      <th>3808</th>\n      <td>Okay, I know I've been crying easily today, bu...</td>\n      <td>Okay. Oh, uh, you dropped something. Jim?</td>\n      <td>Alright, I'm gonna go warm up the car.</td>\n      <td>Night.</td>\n      <td>Good night.</td>\n      <td>Good night Andy.</td>\n      <td>Bye.</td>\n      <td>Night.</td>\n    </tr>\n    <tr>\n      <th>896</th>\n      <td>Hey, Ryan, are you ok?</td>\n      <td>Yes! How are you going to cook every meal of t...</td>\n      <td>Three kitchens?</td>\n      <td>Most apartments these days have like three.</td>\n      <td>It's actually...</td>\n      <td>Wow, you got totally taken for a ride Beesley.</td>\n      <td>I have one kitchen.</td>\n      <td>And how many kitchens?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# create dataset suitable for our model\ndef construct_conv(row, tokenizer, eos = True):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(reversed([tokenizer.encode(x) + [tokenizer.eos_token_id] for x in row]))\n    conv = flatten(conv)\n    return conv\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n\n        directory = args.cache_dir\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            logger.info(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.examples = pickle.load(handle)\n        else:\n            logger.info(\"Creating features from dataset file at %s\", directory)\n\n            self.examples = []\n            for _, row in df.iterrows():\n                conv = construct_conv(row, tokenizer)\n                self.examples.append(conv)\n\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.569067Z","iopub.execute_input":"2021-09-12T15:21:38.569253Z","iopub.status.idle":"2021-09-12T15:21:38.579916Z","shell.execute_reply.started":"2021-09-12T15:21:38.569231Z","shell.execute_reply":"2021-09-12T15:21:38.579284Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Cacheing and storing of data/checkpoints\n\ndef load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n    ordering_and_checkpoint_path = []\n\n    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n\n    for path in glob_checkpoints:\n        if use_mtime:\n            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n        else:\n            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n            if regex_match and regex_match.groups():\n                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n    return checkpoints_sorted\n\n\ndef _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n    if not args.save_total_limit:\n        return\n    if args.save_total_limit <= 0:\n        return\n\n    # Check if we should delete older checkpoint(s)\n    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n    if len(checkpoints_sorted) <= args.save_total_limit:\n        return\n\n    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n    for checkpoint in checkpoints_to_be_deleted:\n        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n        shutil.rmtree(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.581263Z","iopub.execute_input":"2021-09-12T15:21:38.581695Z","iopub.status.idle":"2021-09-12T15:21:38.595602Z","shell.execute_reply.started":"2021-09-12T15:21:38.581657Z","shell.execute_reply":"2021-09-12T15:21:38.594931Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Build the Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:38.596871Z","iopub.execute_input":"2021-09-12T15:21:38.597121Z","iopub.status.idle":"2021-09-12T15:21:56.558750Z","shell.execute_reply.started":"2021-09-12T15:21:38.597091Z","shell.execute_reply":"2021-09-12T15:21:56.557974Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22aacad5d6e94529a13b59e74d115911"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c6211bfdcd84960b72a41c402f62c4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae0c2bf99f5646c8bc2bd06ead86a31d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a373938bb4c6455db4ea2fd52a32072f"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/models/auto/modeling_auto.py:902: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  FutureWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/351M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e89d160e87dd4268b3fe05c3fef37f3a"}},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\nFine-tuning the library models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT and GPT-2 are fine-tuned using a causal language modeling (CLM) loss while BERT and RoBERTa \nare fine-tuned using a masked language modeling (MLM) loss.\n\"\"\"\n\n# Configs\nlogger = logging.getLogger(__name__)\n\nMODEL_CONFIG_CLASSES = list(MODEL_WITH_LM_HEAD_MAPPING.keys())\nMODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:56.560046Z","iopub.execute_input":"2021-09-12T15:21:56.560554Z","iopub.status.idle":"2021-09-12T15:21:56.566046Z","shell.execute_reply.started":"2021-09-12T15:21:56.560516Z","shell.execute_reply":"2021-09-12T15:21:56.565431Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Args to allow for easy convertion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = 'output-small'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = 'microsoft/DialoGPT-small'\n        self.config_name = 'microsoft/DialoGPT-small'\n        self.tokenizer_name = 'microsoft/DialoGPT-small'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.do_train = True\n        self.do_eval = True\n        self.evaluate_during_training = False\n        self.per_gpu_train_batch_size = 4\n        self.per_gpu_eval_batch_size = 4\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 4\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_steps = 3500\n        self.save_total_limit = None\n        self.eval_all_checkpoints = False\n        self.no_cuda = False\n        self.overwrite_output_dir = True\n        self.overwrite_cache = True\n        self.should_continue = False\n        self.seed = 42\n        self.local_rank = -1\n        self.fp16 = False\n        self.fp16_opt_level = 'O1'\n\nargs = Args()","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:56.567449Z","iopub.execute_input":"2021-09-12T15:21:56.568027Z","iopub.status.idle":"2021-09-12T15:21:56.580085Z","shell.execute_reply.started":"2021-09-12T15:21:56.567990Z","shell.execute_reply":"2021-09-12T15:21:56.579414Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Train and Evaluate","metadata":{}},{"cell_type":"code","source":"def train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    \"\"\" Train the model \"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n    # add_special_tokens_(model, tokenizer)\n\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if (\n        args.model_name_or_path\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            inputs, labels = (batch, batch)\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    checkpoint_prefix = \"checkpoint\"\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                    os.makedirs(output_dir, exist_ok=True)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    _rotate_checkpoints(args, checkpoint_prefix)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step\n\n# Evaluation of some model\n\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = args.output_dir\n\n    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n    os.makedirs(eval_output_dir, exist_ok=True)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n\n    def collate(examples: List[torch.Tensor]):\n        if tokenizer._pad_token is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs, labels = (batch, batch)\n        inputs = inputs.to(args.device)\n        labels = labels.to(args.device)\n\n        with torch.no_grad():\n            outputs = model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results {} *****\".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:56.581752Z","iopub.execute_input":"2021-09-12T15:21:56.582250Z","iopub.status.idle":"2021-09-12T15:21:56.656903Z","shell.execute_reply.started":"2021-09-12T15:21:56.582216Z","shell.execute_reply":"2021-09-12T15:21:56.656137Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Main runner\n\ndef main(df_trn, df_val):\n    args = Args()\n    \n    if args.should_continue:\n        sorted_checkpoints = _sorted_checkpoints(args)\n        if len(sorted_checkpoints) == 0:\n            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n        else:\n            args.model_name_or_path = sorted_checkpoints[-1]\n\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n        and not args.should_continue\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\")\n    args.n_gpu = torch.cuda.device_count()\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n\n    # Set seed\n    set_seed(args)\n\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelWithLMHead.from_pretrained(\n        args.model_name_or_path,\n        from_tf=False,\n        config=config,\n        cache_dir=args.cache_dir,\n    )\n    model.to(args.device)\n    \n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Saving best-practices: if you use save_pretrained for the model and tokenizer, you can reload them using from_pretrained()\n    if args.do_train:\n        # Create output directory if needed\n        os.makedirs(args.output_dir, exist_ok=True)\n\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n        # They can then be reloaded using `from_pretrained()`\n        model_to_save = (\n            model.module if hasattr(model, \"module\") else model\n        )  # Take care of distributed/parallel training\n        model_to_save.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n\n        # Good practice: save your training arguments together with the trained model\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Load a trained model and vocabulary that you have fine-tuned\n        model = AutoModelWithLMHead.from_pretrained(args.output_dir)\n        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = AutoModelWithLMHead.from_pretrained(checkpoint)\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n            results.update(result)\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:56.658333Z","iopub.execute_input":"2021-09-12T15:21:56.658664Z","iopub.status.idle":"2021-09-12T15:21:56.679963Z","shell.execute_reply.started":"2021-09-12T15:21:56.658623Z","shell.execute_reply":"2021-09-12T15:21:56.679125Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Run the Main Function","metadata":{}},{"cell_type":"code","source":"main(trn_df, val_df)","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:21:56.681365Z","iopub.execute_input":"2021-09-12T15:21:56.681827Z","iopub.status.idle":"2021-09-12T15:33:47.150959Z","shell.execute_reply.started":"2021-09-12T15:21:56.681790Z","shell.execute_reply":"2021-09-12T15:33:47.150027Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9895c44f86f4b5db7adc9ab3bfc3bc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8d6a86666a54701a5036a0db9955288"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d239885824a2440ebb2af6d1771272a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f68e1a453ea4ff6a90beca860adeab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/351M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2e08ddd63ab4a679fe84985b94af3d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5805350bca7410f82db03d46b680999"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa93292197b749e7874a5077a15ef4f0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a7004fdb4e34951b1cd255659a3e11f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79972bd33cd1458caaa9c7a033aec4e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/1118 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67f5f78805644bdb8b2d15523d85f1d2"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/124 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f544225b6c04c87810244a2edfee58e"}},"metadata":{}},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"{'perplexity_': tensor(6.9946)}"},"metadata":{}}]},{"cell_type":"markdown","source":"# Load the Trained Model","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\nmodel = AutoModelWithLMHead.from_pretrained('output-small')","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:39:02.119676Z","iopub.execute_input":"2021-09-12T15:39:02.119946Z","iopub.status.idle":"2021-09-12T15:39:08.269642Z","shell.execute_reply.started":"2021-09-12T15:39:02.119920Z","shell.execute_reply":"2021-09-12T15:39:08.268890Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Let's chat for 4 lines\nfor step in range(4):\n    # encode the new user input, add the eos_token and return a tensor in Pytorch\n    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n    # print(new_user_input_ids)\n\n    # append the new user input tokens to the chat history\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    # generated a response while limiting the total chat history to 1000 tokens, \n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,  \n        no_repeat_ngram_size=3,       \n        do_sample=True, \n        top_k=100, \n        top_p=0.7,\n        temperature=0.8\n    )\n    \n    # pretty print last ouput tokens from bot\n    print(\"AliciaBot: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:40:09.194659Z","iopub.execute_input":"2021-09-12T15:40:09.194922Z","iopub.status.idle":"2021-09-12T15:40:41.789574Z","shell.execute_reply.started":"2021-09-12T15:40:09.194894Z","shell.execute_reply":"2021-09-12T15:40:41.788819Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdin","text":">> User: hi alicia\n"},{"name":"stdout","text":"AliciaBot: Hi.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: how michael?\n"},{"name":"stdout","text":"AliciaBot: Hi, I'm new here. I just wanted to say hi.\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: how's michael?\n"},{"name":"stdout","text":"AliciaBot: How are you?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":">> User: yeah i'm fine, you?\n"},{"name":"stdout","text":"AliciaBot: !!!'!! '!!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Push Model to Hugging Face","metadata":{}},{"cell_type":"code","source":"!pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:41:44.798022Z","iopub.execute_input":"2021-09-12T15:41:44.798295Z","iopub.status.idle":"2021-09-12T15:41:52.326974Z","shell.execute_reply.started":"2021-09-12T15:41:44.798268Z","shell.execute_reply":"2021-09-12T15:41:52.325953Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: huggingface_hub in /opt/conda/lib/python3.7/site-packages (0.0.12)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (21.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.7.4.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.0.12)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (3.4.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (2.25.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface_hub) (4.62.1)\nRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface_hub) (2.4.7)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface_hub) (3.5.0)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (4.0.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2021.5.30)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface_hub) (1.26.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"!huggingface-cli login","metadata":{"execution":{"iopub.status.busy":"2021-09-12T15:44:15.222303Z","iopub.execute_input":"2021-09-12T15:44:15.222847Z","iopub.status.idle":"2021-09-12T15:44:41.300519Z","shell.execute_reply.started":"2021-09-12T15:44:15.222806Z","shell.execute_reply":"2021-09-12T15:44:41.299572Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n\n        \nUsername: ^C\nTraceback (most recent call last):\n  File \"/opt/conda/bin/huggingface-cli\", line 8, in <module>\n    sys.exit(main())\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 41, in main\n    service.run()\n  File \"/opt/conda/lib/python3.7/site-packages/huggingface_hub/commands/user.py\", line 138, in run\n    username = input(\"Username: \")\nKeyboardInterrupt\n","output_type":"stream"}]},{"cell_type":"code","source":"!git config --global user.email \"lynnzheng08@outlook.com\"\n# Tip: using the same email as your huggingface.co account will link your commits to your profile\n!git config --global user.name \"Lynn Zheng\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MY_MODEL_NAME = 'DialoGPT-small-joshua'\nwith open('HuggingFace-API-key.txt', 'rt') as f:\n  HUGGINGFACE_API_KEY = f.read().strip()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.push_to_hub(MY_MODEL_NAME, use_auth_token=HUGGINGFACE_API_KEY)\ntokenizer.push_to_hub(MY_MODEL_NAME, use_auth_token=HUGGINGFACE_API_KEY)","metadata":{},"execution_count":null,"outputs":[]}]}